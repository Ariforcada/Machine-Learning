# -*- coding: utf-8 -*-
"""Assignment_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tgkGfhMkVegCZ_o6SW8jWXoBNhC4-gHa

**EXERCISE 2**

Preliminary step: Download required packages
"""

!pip install vaderSentiment
import math
import pandas as pd
import numpy as np
import re
import gensim
import seaborn as sns
from datetime import datetime
from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS
from gensim.models.word2vec import Text8Corpus
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import seaborn as sns
import matplotlib.pyplot as plt
from gensim.test.utils import datapath
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import string

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from google.colab import drive
drive.mount('/content/drive')

"""**1. Read the file NYT headline.csv on python and drop the duplicates**

If the file is not contained in the Drive Colab Notebooks of the user, please change the path to include the path where the cvs file is in order to read it.

When reading the data, we use index_colto have the first column of the CSV file as the row index.
"""

path = 'drive/My Drive/Colab Notebooks/NYT_headlines.csv'
nyt_data = pd.read_csv(path, index_col=0)

# Drop duplicates
nyt_data_cleaned = nyt_data.drop_duplicates()
print(nyt_data_cleaned.head())

"""**2. Combine the different headlines by day**

The months are either 'Feb.' or 'March', but we are interested in putting in the format '%Y %b %d'.  In order to do so we first correct the wording of the month values so they are properly read. We also include the year (2021) and set the index to date
"""

# Mapping function to replace month abbreviations
month_mapping = {
    'Feb.': 'Feb',
    'March': 'Mar',
}

# Apply the mapping to replace month abbreviations and add year 2021 to each date string
nyt_data_cleaned.loc[:,'date'] = '2021 ' + nyt_data_cleaned['date']

# Convert the 'date' column to datetime
nyt_data_cleaned.loc[:,'date'] = nyt_data_cleaned['date'].replace(month_mapping, regex=True)
nyt_data_cleaned.loc[:,'date'] = pd.to_datetime(nyt_data_cleaned['date'], format='%Y %b %d')

# Set the index to 'date', group by 'date' and combine headlines for each day
nyt_data_cleaned.set_index('date', inplace=True)
nyt_data_grouped = nyt_data_cleaned.groupby('date')['Headlines'].apply(' '.join).reset_index()
print(nyt_data_grouped.head())

"""**3. Using a heatmap, show the day that are close to each other in terms of similarity in the news discussed.**"""

# Preprocess the 'headlines' column: lowercases, remove non-alphabetic characters and stop words and lemmatize

# Initialize lemmatizer and stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def clean_detail(doc):
  doc1= doc.lower()
  doc2= doc1.split()
  doc3= [word for word in doc2 if word not in stop_words]
  doc4= ' '.join(doc3)
  doc5= ''.join([char for char in doc4 if not char.isdigit()])
  doc6= ''.join([char for char in doc5])
  doc7=[val for val in doc6 if val not in string.punctuation]
  doc8= ''.join([val for val in doc7])
  doc9 = doc8.split()
  doc10 = [lemmatizer.lemmatize(val) for val in doc9]
  doc11 = ' '.join(val for val in doc10)
  return doc11

# Apply cleaning to all headlines
nyt_data_grouped['cleaned_headlines'] = nyt_data_grouped['Headlines'].apply(clean_detail)
print(nyt_data_grouped.head())

# Detect collocations (bigrams) using Gensim's Phrases model

# Tokenize the headlines into words
tokenized_headlines = [headline.split() for headline in nyt_data_grouped['cleaned_headlines']]

# Train a collocation detection model (using bigrams) with gensim's Phrases
phrase_model = Phrases(tokenized_headlines, min_count=3, threshold=6, connector_words=ENGLISH_CONNECTOR_WORDS)

# Apply the trained collocation model to the tokenized headlines to merge collocations
bigram_headlines = [phrase_model[headline] for headline in tokenized_headlines]

#Tune min_count and threshold parameters by looking at the number and values of headlines
collocation_count = 0
for headline in bigram_headlines:
    collocations = [bigram for bigram in headline if '_' in bigram]
    #print(collocations)
    collocation_count += len(collocations)
print("Total number of collocations/bigrams", collocation_count)

# Reprocess the headlines with bigrams, and create a single string for each headline
nyt_data_grouped['processed_headline'] = [' '.join(headline) for headline in bigram_headlines]

# Group the cleaned headlines by date (same as before, but adding the processed headlines now)
# Group by 'date' and aggregate the headlines, keeping all columns intact
nyt_data_grouped = nyt_data_grouped.groupby('date').agg({
    'cleaned_headlines': ' '.join,  # Join all cleaned headlines by space
    'processed_headline': ' '.join  # Join all processed headlines (bigrams) by space
}).reset_index()
print(nyt_data_grouped.head())

#Create the TF-IDF matrix

# Create a TfidfVectorizer to transform the headlines into a TF-IDF matrix
vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(nyt_data_grouped['processed_headline'])
print("Shape of the TF-IDF matrix:", tfidf_matrix.shape)
feature_names = vectorizer.get_feature_names_out()

# Calculate the cosine similarity between days
cosine_sim = cosine_similarity(tfidf_matrix)

# Create a heatmap for the similarity matrix
# Convert the cosine similarity matrix into a DataFrame for better readability
similarity_df = pd.DataFrame(cosine_sim, index=nyt_data_grouped['date'], columns=nyt_data_grouped['date'])

# Convert the index and columns of the DataFrame to just the date (remove time)
similarity_df.index = similarity_df.index.date  # Remove time from the index
similarity_df.columns = similarity_df.columns.date  # Remove time from the columns

# Plot the heatmap using seaborn without annotations and with the diagonal masked
plt.figure(figsize=(10, 8))
plt.title('Cosine Similarity Heatmap of News Headlines by Day')
mask = np.eye(len(similarity_df), dtype=bool)
sns.heatmap(similarity_df,
            annot=False,
            cmap='coolwarm',
            mask=mask,
            cbar_kws={'label': 'Similarity'})
plt.xlabel('Date')
plt.ylabel('Date')
plt.show()

"""Enhanced similarity visualizations: triangular heatmap and clustermap"""

# Triangular (lower) heatmap to focus on unique pairs
mask_triangular = np.triu(np.ones_like(similarity_df, dtype=bool))
plt.figure(figsize=(10, 8))
plt.title('Cosine Similarity (Lower Triangle) of News Headlines by Day')
sns.heatmap(
    similarity_df,
    mask=mask_triangular,
    cmap='coolwarm',
    annot=False,
    square=True,
    linewidths=0.5,
    cbar_kws={'label': 'Similarity'}
)
plt.xlabel('Date')
plt.ylabel('Date')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# Clustered heatmap with dendrograms to reveal day groupings
try:
    clustered = sns.clustermap(
        similarity_df,
        cmap='coolwarm',
        figsize=(10, 10),
        cbar_kws={'label': 'Similarity'}
    )
    plt.setp(clustered.ax_heatmap.get_xticklabels(), rotation=45, ha='right')
    plt.setp(clustered.ax_heatmap.get_yticklabels(), rotation=0)
    clustered.ax_heatmap.set_title('Clustered Cosine Similarity of News Headlines by Day')
    plt.show()
except Exception as clustering_error:
    # If scipy linkage is unavailable, skip clustering to keep script runnable
    print(f"Clustermap skipped due to: {clustering_error}")

"""Find the 3 pairs of dates with the most textual similarity"""

# Remove the diagonal (same-date comparisons)
similarity_stacked = similarity_df.stack()
similarity_stacked = similarity_stacked[similarity_stacked.index.get_level_values(0) != similarity_stacked.index.get_level_values(1)]

# Get unique pairs (only keep one direction of the pair)
# Sort the pairs by the date values to ensure (A, B) is the same as (B, A)
similarity_stacked.index = [tuple(sorted(pair)) for pair in similarity_stacked.index]

# Now we can remove duplicates by just keeping the first occurrence of each pair
similarity_stacked = similarity_stacked.groupby(similarity_stacked.index).max()

# Sort by similarity value in descending order
top_pairs = similarity_stacked.sort_values(ascending=False).head(3)

# Print top pairs and their similarity values
print(top_pairs)

specific_dates = ['2021-02-03', '2021-02-04', '2021-02-08', '2021-02-09', '2021-02-13']
specific_dates_articles = nyt_data_grouped[nyt_data_grouped['date'].isin(specific_dates)]
print(specific_dates_articles[['date', 'cleaned_headlines']])

"""**4. Build a vocabulary of Covid-19 related words**"""

# Step 4: Predefine Covid-19 related words (expand the list as needed)
covid_related_words = [
    "covid", "covid-19", "coronavirus", "pandemic", "virus", "quarantine",
    "lockdown", "vaccine", "vaccination", "infection", "cases", "symptoms",
    "distancing", "health", "mask", "immunity", "variants",
    "omicron", "delta", "transmission", "testing", "treatment","hospitalization",
    "healthcare", "caregiver","boosters"
]

"""**5. Using the vocabulary constructed, build a daily covid related index by estimating the relative fraction of articles related to covid to the total number of articles per day**"""

# Tokenize the headlines and search for Covid-related words
def contains_covid_related_terms(headline):
    # Tokenize the headline into words and check if any of the covid-related words appear
    words = headline.split()
    for word in words:
        if word in covid_related_words:
            return True
    return False

# Apply previously defined cleaning function to all headlines
nyt_data_cleaned.loc[:,'cleaned_sep_headlines'] = nyt_data_cleaned['Headlines'].apply(clean_detail)

# Count how many articles per day are related to Covid
nyt_data_cleaned.loc[:,'covid_related'] = nyt_data_cleaned['cleaned_sep_headlines'].apply(contains_covid_related_terms)

nyt_covid_grouped = nyt_data_cleaned.groupby('date').agg(
    total_articles=('covid_related', 'count'),  # Total number of articles per day
    covid_articles=('covid_related', 'sum')     # Total number of Covid-related articles per day
).reset_index()

# Calculate the Covid Uncertainty Index (fraction of Covid-related articles)
nyt_covid_grouped['covid_uncertainty_index'] = nyt_covid_grouped['covid_articles'] / nyt_covid_grouped['total_articles']

# Print the result to see the Covid Uncertainty Index for each day
print(nyt_covid_grouped[['date', 'covid_uncertainty_index']].head())

# Plot the Covid Uncertainty Index over time with 7-day smoothing
nyt_covid_grouped = nyt_covid_grouped.sort_values('date')
nyt_covid_grouped['covid_index_ma7'] = nyt_covid_grouped['covid_uncertainty_index'].rolling(window=7, min_periods=1).mean()

plt.figure(figsize=(10, 6))
plt.plot(nyt_covid_grouped['date'], nyt_covid_grouped['covid_uncertainty_index'], label='Daily', color='tab:blue', alpha=0.4)
plt.plot(nyt_covid_grouped['date'], nyt_covid_grouped['covid_index_ma7'], label='7-day MA', color='tab:blue')
plt.title('Covid-Related Article Share Over Time', fontsize=12)
plt.xlabel('Date', fontsize=10)
plt.ylabel('Covid Article Share', fontsize=10)
plt.legend()
plt.grid(True, axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

"""**6. Construct a daily economic policy uncertainty index.
Build the current index by estimating the relative fraction of articles that use any of those words**
"""

# Define the Economic Policy Uncertainty (EPU) related words
economic_policy_words = [
    "uncertainty", "uncertain", "economic", "economy", "congress", "deficit",
    "federal reserve", "legislation", "regulation", "white house", "uncertainties",
    "regulatory", "the fed"
]

sep_words = {"federal": "reserve", "white": "house", "the": "fed"}

# Tokenize the headlines and check for EPU-related words
def contains_epu_related_terms(headline):
    # Tokenize the headline into words and check if any of the EPU-related words appear
    words = headline.split()
    for i, word in enumerate(words):
        if word in economic_policy_words:
            return True
        if word in sep_words:
            if i + 1 < len(words) and sep_words[word] == words[i+1]:
                return True
    return False

# Count how many articles per day are related to Covid
nyt_data_cleaned.loc[:,'epu_related'] = nyt_data_cleaned['cleaned_sep_headlines'].apply(contains_epu_related_terms)

nyt_epu_grouped = nyt_data_cleaned.groupby('date').agg(
    total_articles=('epu_related', 'count'),  # Total number of articles per day
    epu_articles=('epu_related', 'sum')    # Total number of Covid-related articles per day
).reset_index()

# Calculate the Covid Uncertainty Index (fraction of Covid-related articles)
nyt_epu_grouped['epu_uncertainty_index'] = nyt_epu_grouped['epu_articles'] / nyt_epu_grouped['total_articles']

# Display the Economic Policy Uncertainty Index
print(nyt_epu_grouped[['date','epu_uncertainty_index']].head())

# Plot the EPU Index over time with 7-day smoothing
nyt_epu_grouped = nyt_epu_grouped.sort_values('date')
nyt_epu_grouped['epu_index_ma7'] = nyt_epu_grouped['epu_uncertainty_index'].rolling(window=7, min_periods=1).mean()

plt.figure(figsize=(10, 6))
plt.plot(nyt_epu_grouped['date'], nyt_epu_grouped['epu_uncertainty_index'], label='Daily', color='tab:orange', alpha=0.4)
plt.plot(nyt_epu_grouped['date'], nyt_epu_grouped['epu_index_ma7'], label='7-day MA', color='tab:orange')
plt.title('Economic Policy Uncertainty Article Share Over Time', fontsize=12)
plt.xlabel('Date', fontsize=10)
plt.ylabel('EPU Article Share', fontsize=10)
plt.legend()
plt.grid(True, axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# Combined overlay: Covid vs EPU (smoothed)
merged_overlay = pd.merge(nyt_covid_grouped[['date','covid_index_ma7']],
                          nyt_epu_grouped[['date','epu_index_ma7']], on='date', how='inner')
plt.figure(figsize=(10, 6))
plt.plot(merged_overlay['date'], merged_overlay['covid_index_ma7'], label='Covid (7-day MA)', color='tab:blue')
plt.plot(merged_overlay['date'], merged_overlay['epu_index_ma7'], label='EPU (7-day MA)', color='tab:orange')
plt.title('Covid vs EPU Article Share (7-day Smoothed)')
plt.xlabel('Date')
plt.ylabel('Share of Articles')
plt.legend()
plt.grid(True, axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

"""**8. Use the variable “Adj Close” to compute the return on S&P500 (GSPC)**"""

SP500 = pd.read_csv('drive/My Drive/Colab Notebooks/SP500.csv')
SP500.rename(columns={'Date': 'date'}, inplace=True)
SP500['date'] = pd.to_datetime(SP500['date'])
print(SP500.head())

# Sort the data by date to ensure proper calculation of returns
SP500.sort_values(by='date', ascending=True, inplace=True)
SP500['Adj Close**'] = SP500['Adj Close**'].replace({r'[,\$]': ''}, regex=True)
SP500['Adj Close**'] = pd.to_numeric(SP500['Adj Close**'])

# Calculate the daily returns using the 'Adj Close' column
SP500['Return'] = SP500['Adj Close**'].pct_change()*100

# Check the first few rows to verify the calculations
print(SP500[['date', 'Adj Close**', 'Return']].head())

results_df = pd.merge(pd.merge(nyt_covid_grouped, nyt_epu_grouped, on='date', how='inner'), SP500, on='date', how='inner')
results_df.set_index('date', inplace=True)

# Compute correlations between the three variables
correlation_matrix = results_df[['covid_uncertainty_index', 'epu_uncertainty_index', 'Return']].corr()

# Plot the correlation matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)
plt.title('Correlation Matrix between Covid Uncertainty Index, CEPI, and Returns')
plt.show()

# Distributions of indices and returns
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
sns.histplot(results_df['covid_uncertainty_index'].dropna(), kde=True, ax=axes[0], color='tab:blue')
axes[0].set_title('Covid Index Distribution')
axes[0].set_xlabel('Covid Article Share')

sns.histplot(results_df['epu_uncertainty_index'].dropna(), kde=True, ax=axes[1], color='tab:orange')
axes[1].set_title('EPU Index Distribution')
axes[1].set_xlabel('EPU Article Share')

sns.histplot(results_df['Return'].dropna(), kde=True, ax=axes[2], color='tab:green')
axes[2].set_title('S&P 500 Returns Distribution')
axes[2].set_xlabel('Daily Return (%)')

plt.tight_layout()
plt.show()

# Plot the variables over time to visually inspect their relationships
plt.figure(figsize=(14, 8))

# Plot Covid Uncertainty Index
plt.subplot(3, 1, 1)
plt.plot(results_df.index, results_df['covid_uncertainty_index'], label='Covid Uncertainty Index', color='b')
plt.title('Covid Uncertainty Index Over Time')
plt.xlabel('Date')
plt.ylabel('Index Value')
plt.legend()

# Plot CEPI
plt.subplot(3, 1, 2)
plt.plot(results_df.index, results_df['epu_uncertainty_index'], label='CEPI', color='g')
plt.title('Coarse Economic Policy Index Over Time')
plt.xlabel('Date')
plt.ylabel('Index Value')
plt.legend()

# Plot Returns
plt.subplot(3, 1, 3)
plt.plot(results_df.index, results_df['Return'], label='Return', color='r')
plt.title('Returns Over Time')
plt.xlabel('Date')
plt.ylabel('Returns')
plt.legend()

plt.tight_layout()
plt.show()

# Scatter/regression and joint density plots
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Regression plots (with lowess trend via seaborn's regplot if available)
sns.regplot(ax=axes[0], x=results_df['covid_uncertainty_index'], y=results_df['Return'], scatter_kws={'alpha': 0.5}, line_kws={'color':'tab:blue'})
axes[0].set_title('Return vs Covid Index')
axes[0].set_xlabel('Covid Article Share')
axes[0].set_ylabel('S&P 500 Daily Return (%)')

sns.regplot(ax=axes[1], x=results_df['epu_uncertainty_index'], y=results_df['Return'], scatter_kws={'alpha': 0.5}, line_kws={'color':'tab:orange'})
axes[1].set_title('Return vs EPU Index')
axes[1].set_xlabel('EPU Article Share')
axes[1].set_ylabel('S&P 500 Daily Return (%)')

plt.tight_layout()
plt.show()

try:
    # Joint density plots with KDE contours
    g1 = sns.jointplot(x='covid_uncertainty_index', y='Return', data=results_df, kind='kde', fill=True, height=5, cmap='Blues')
    g1.fig.suptitle('Joint Density: Return vs Covid Index')
    plt.tight_layout()
    plt.show()

    g2 = sns.jointplot(x='epu_uncertainty_index', y='Return', data=results_df, kind='kde', fill=True, height=5, cmap='Oranges')
    g2.fig.suptitle('Joint Density: Return vs EPU Index')
    plt.tight_layout()
    plt.show()
except Exception as e:
    print(f"Joint density plots skipped: {e}")

# Rolling correlations between indices and S&P 500 returns
window_days = 14
rolling_corr = pd.DataFrame({
    'corr_return_covid': results_df['Return'].rolling(window_days, min_periods=5).corr(results_df['covid_uncertainty_index']),
    'corr_return_epu': results_df['Return'].rolling(window_days, min_periods=5).corr(results_df['epu_uncertainty_index'])
})

plt.figure(figsize=(12, 6))
plt.plot(rolling_corr.index, rolling_corr['corr_return_covid'], label=f'Return vs Covid (roll {window_days}d)', color='tab:blue')
plt.plot(rolling_corr.index, rolling_corr['corr_return_epu'], label=f'Return vs EPU (roll {window_days}d)', color='tab:orange')
plt.axhline(0, color='black', linewidth=1, linestyle='--', alpha=0.5)
plt.title('Rolling Correlations between S&P 500 Returns and News Indices')
plt.xlabel('Date')
plt.ylabel('Correlation')
plt.legend()
plt.grid(True, axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

import nltk
nltk.download('vader_lexicon')
from nltk.sentiment import SentimentIntensityAnalyzer

covid_sentiment_df = nyt_data_cleaned.copy()
covid_sentiment_df = covid_sentiment_df.reset_index()
covid_sentiment_df['date'] = pd.to_datetime(covid_sentiment_df['date'], format='%Y %b %d')

# Initialize VADER Sentiment Analyzer
analyzer = SentimentIntensityAnalyzer()

# Function to get sentiment scores (Negative, Neutral, Positive)
def get_sentiment_scores(text):
    sentiment = analyzer.polarity_scores(text)
    return sentiment['neg'], sentiment['neu'], sentiment['pos']

# Apply sentiment analysis to the filtered dataframe
covid_sentiment_df[['negative', 'neutral', 'positive']] = covid_sentiment_df['cleaned_sep_headlines'].apply(lambda x: pd.Series(get_sentiment_scores(x)))

full_date_range = pd.date_range(start=covid_sentiment_df['date'].min(), end=covid_sentiment_df['date'].max(), freq='D')
# Aggregate sentiment scores by date
daily_sentiment = covid_sentiment_df.groupby('date')[['negative', 'neutral', 'positive']].mean()
daily_sentiment = daily_sentiment.reindex(full_date_range)

covid_sentiment_covid_df = nyt_data_cleaned[nyt_data_cleaned['covid_related']]
covid_sentiment_covid_df = covid_sentiment_covid_df.reset_index()
covid_sentiment_covid_df['date'] = pd.to_datetime(covid_sentiment_covid_df['date'], format='%Y %b %d')

# Apply sentiment analysis to the filtered dataframe
covid_sentiment_covid_df[['negative', 'neutral', 'positive']] = covid_sentiment_covid_df['cleaned_sep_headlines'].apply(lambda x: pd.Series(get_sentiment_scores(x)))

# Aggregate sentiment scores by date
daily_sentiment_covid = covid_sentiment_covid_df.groupby('date')[['negative', 'neutral', 'positive']].mean()
daily_sentiment_covid = daily_sentiment_covid.reindex(full_date_range)

# Plotting the daily sentiment scores
plt.figure(figsize=(12, 6))

# 7-day rolling averages for smoother sentiment trends
sent_ma = daily_sentiment_covid.rolling(window=7, min_periods=1).mean()

plt.plot(daily_sentiment_covid.index, daily_sentiment_covid['negative'], label='Negative (Daily)', color='red', alpha=0.2)
plt.plot(daily_sentiment_covid.index, daily_sentiment_covid['neutral'], label='Neutral (Daily)', color='gray', alpha=0.2)
plt.plot(daily_sentiment_covid.index, daily_sentiment_covid['positive'], label='Positive (Daily)', color='green', alpha=0.2)

plt.plot(sent_ma.index, sent_ma['negative'], label='Negative (7d MA)', color='red')
plt.plot(sent_ma.index, sent_ma['neutral'], label='Neutral (7d MA)', color='gray')
plt.plot(sent_ma.index, sent_ma['positive'], label='Positive (7d MA)', color='green')

# Customizing the plot
plt.title('Daily Sentiment Analysis of Covid-Related Articles')
plt.xlabel('Date')
plt.ylabel('Sentiment Score')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()

# Show the plot
plt.show()

# Stacked area chart for sentiment composition
plt.figure(figsize=(12, 6))
plt.stackplot(daily_sentiment_covid.index,
              daily_sentiment_covid['negative'],
              daily_sentiment_covid['neutral'],
              daily_sentiment_covid['positive'],
              labels=['Negative', 'Neutral', 'Positive'],
              colors=['red', 'gray', 'green'],
              alpha=0.6)
plt.title('Daily Sentiment Composition (Covid-Related Articles)')
plt.xlabel('Date')
plt.ylabel('Average Sentiment Score')
plt.legend(loc='upper left')
plt.grid(True, axis='y', alpha=0.3)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Net sentiment and polarity balance
net_sentiment = daily_sentiment_covid['positive'] - daily_sentiment_covid['negative']
polarity_balance = (daily_sentiment_covid['positive'] - daily_sentiment_covid['negative']) / (daily_sentiment_covid[['positive','negative']].sum(axis=1).replace(0, np.nan))

plt.figure(figsize=(12, 4))
plt.plot(net_sentiment.index, net_sentiment, label='Net Sentiment (pos - neg)', color='tab:purple')
plt.axhline(0, color='black', linewidth=1, linestyle='--', alpha=0.5)
plt.title('Net Sentiment Over Time')
plt.xlabel('Date')
plt.ylabel('Net Sentiment')
plt.legend()
plt.grid(True, axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# Aggregate sentiment over the entire period (mean of negative, neutral, and positive)
aggregate_sentiment = covid_sentiment_df[['negative', 'neutral', 'positive']].mean()
print(aggregate_sentiment)

# Plotting the aggregate sentiment scores as a bar plot
aggregate_sentiment.plot(kind='bar', color=['red', 'gray', 'green'], figsize=(6, 4))

# Customizing the plot
plt.title('Aggregate Sentiment for Covid-Related Articles (Entire Period)')
plt.ylabel('Average Sentiment Score')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()