# -*- coding: utf-8 -*-
"""Assignment_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tgkGfhMkVegCZ_o6SW8jWXoBNhC4-gHa

**EXERCISE 2**

Preliminary step: Download required packages
"""

!pip install vaderSentiment
import math
import pandas as pd
import numpy as np
import re
import gensim
import seaborn as sns
from datetime import datetime
from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS
from gensim.models.word2vec import Text8Corpus
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import seaborn as sns
import matplotlib.pyplot as plt
from gensim.test.utils import datapath
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import string

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from google.colab import drive
drive.mount('/content/drive')

"""**1. Read the file NYT headline.csv on python and drop the duplicates**

If the file is not contained in the Drive Colab Notebooks of the user, please change the path to include the path where the cvs file is in order to read it.

When reading the data, we use index_colto have the first column of the CSV file as the row index.
"""

path = 'drive/My Drive/Colab Notebooks/NYT_headlines.csv'
nyt_data = pd.read_csv(path, index_col=0)

# Drop duplicates
nyt_data_cleaned = nyt_data.drop_duplicates()
print(nyt_data_cleaned.head())

"""**2. Combine the different headlines by day**

The months are either 'Feb.' or 'March', but we are interested in putting in the format '%Y %b %d'.  In order to do so we first correct the wording of the month values so they are properly read. We also include the year (2021) and set the index to date
"""

# Mapping function to replace month abbreviations
month_mapping = {
    'Feb.': 'Feb',
    'March': 'Mar',
}

# Apply the mapping to replace month abbreviations and add year 2021 to each date string
nyt_data_cleaned.loc[:,'date'] = '2021 ' + nyt_data_cleaned['date']

# Convert the 'date' column to datetime
nyt_data_cleaned.loc[:,'date'] = nyt_data_cleaned['date'].replace(month_mapping, regex=True)
nyt_data_cleaned.loc[:,'date'] = pd.to_datetime(nyt_data_cleaned['date'], format='%Y %b %d')

# Set the index to 'date', group by 'date' and combine headlines for each day
nyt_data_cleaned.set_index('date', inplace=True)
nyt_data_grouped = nyt_data_cleaned.groupby('date')['Headlines'].apply(' '.join).reset_index()
print(nyt_data_grouped.head())

"""**3. Using a heatmap, show the day that are close to each other in terms of similarity in the news discussed.**"""

# Preprocess the 'headlines' column: lowercases, remove non-alphabetic characters and stop words and lemmatize

# Initialize lemmatizer and stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def clean_detail(doc):
  doc1= doc.lower()
  doc2= doc1.split()
  doc3= [word for word in doc2 if word not in stop_words]
  doc4= ' '.join(doc3)
  doc5= ''.join([char for char in doc4 if not char.isdigit()])
  doc6= ''.join([char for char in doc5])
  doc7=[val for val in doc6 if val not in string.punctuation]
  doc8= ''.join([val for val in doc7])
  doc9 = doc8.split()
  doc10 = [lemmatizer.lemmatize(val) for val in doc9]
  doc11 = ' '.join(val for val in doc10)
  return doc11

# Apply cleaning to all headlines
nyt_data_grouped['cleaned_headlines'] = nyt_data_grouped['Headlines'].apply(clean_detail)
print(nyt_data_grouped.head())

# Detect collocations (bigrams) using Gensim's Phrases model

# Tokenize the headlines into words
tokenized_headlines = [headline.split() for headline in nyt_data_grouped['cleaned_headlines']]

# Train a collocation detection model (using bigrams) with gensim's Phrases
phrase_model = Phrases(tokenized_headlines, min_count=3, threshold=6, connector_words=ENGLISH_CONNECTOR_WORDS)

# Apply the trained collocation model to the tokenized headlines to merge collocations
bigram_headlines = [phrase_model[headline] for headline in tokenized_headlines]

#Tune min_count and threshold parameters by looking at the number and values of headlines
collocation_count = 0
for headline in bigram_headlines:
    collocations = [bigram for bigram in headline if '_' in bigram]
    #print(collocations)
    collocation_count += len(collocations)
print("Total number of collocations/bigrams", collocation_count)

# Reprocess the headlines with bigrams, and create a single string for each headline
nyt_data_grouped['processed_headline'] = [' '.join(headline) for headline in bigram_headlines]

# Group the cleaned headlines by date (same as before, but adding the processed headlines now)
# Group by 'date' and aggregate the headlines, keeping all columns intact
nyt_data_grouped = nyt_data_grouped.groupby('date').agg({
    'cleaned_headlines': ' '.join,  # Join all cleaned headlines by space
    'processed_headline': ' '.join  # Join all processed headlines (bigrams) by space
}).reset_index()
print(nyt_data_grouped.head())

#Create the TF-IDF matrix

# Create a TfidfVectorizer to transform the headlines into a TF-IDF matrix
vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(nyt_data_grouped['processed_headline'])
print("Shape of the TF-IDF matrix:", tfidf_matrix.shape)
feature_names = vectorizer.get_feature_names_out()

# Calculate the cosine similarity between days
cosine_sim = cosine_similarity(tfidf_matrix)

# Create a heatmap for the similarity matrix
# Convert the cosine similarity matrix into a DataFrame for better readability
similarity_df = pd.DataFrame(cosine_sim, index=nyt_data_grouped['date'], columns=nyt_data_grouped['date'])

# Convert the index and columns of the DataFrame to just the date (remove time)
similarity_df.index = similarity_df.index.date  # Remove time from the index
similarity_df.columns = similarity_df.columns.date  # Remove time from the columns

# Plot the heatmap using seaborn without annotations and with the diagonal masked
plt.figure(figsize=(10, 8))
plt.title('Cosine Similarity Heatmap of News Headlines by Day')
mask = np.eye(len(similarity_df), dtype=bool)
sns.heatmap(similarity_df,
            annot=False,
            cmap='coolwarm',
            mask=mask,
            cbar_kws={'label': 'Similarity'})
plt.xlabel('Date')
plt.ylabel('Date')
plt.show()

"""Find the 3 pairs of dates with the most textual similarity"""

# Remove the diagonal (same-date comparisons)
similarity_stacked = similarity_df.stack()
similarity_stacked = similarity_stacked[similarity_stacked.index.get_level_values(0) != similarity_stacked.index.get_level_values(1)]

# Get unique pairs (only keep one direction of the pair)
# Sort the pairs by the date values to ensure (A, B) is the same as (B, A)
similarity_stacked.index = [tuple(sorted(pair)) for pair in similarity_stacked.index]

# Now we can remove duplicates by just keeping the first occurrence of each pair
similarity_stacked = similarity_stacked.groupby(similarity_stacked.index).max()

# Sort by similarity value in descending order
top_pairs = similarity_stacked.sort_values(ascending=False).head(3)

# Print top pairs and their similarity values
print(top_pairs)

specific_dates = ['2021-02-03', '2021-02-04', '2021-02-08', '2021-02-09', '2021-02-13']
specific_dates_articles = nyt_data_grouped[nyt_data_grouped['date'].isin(specific_dates)]
print(specific_dates_articles[['date', 'cleaned_headlines']])

"""**4. Build a vocabulary of Covid-19 related words**"""

# Step 4: Predefine Covid-19 related words (expand the list as needed)
covid_related_words = [
    "covid", "covid-19", "coronavirus", "pandemic", "virus", "quarantine",
    "lockdown", "vaccine", "vaccination", "infection", "cases", "symptoms",
    "distancing", "health", "mask", "immunity", "variants",
    "omicron", "delta", "transmission", "testing", "treatment","hospitalization",
    "healthcare", "caregiver","boosters"
]

"""**5. Using the vocabulary constructed, build a daily covid related index by estimating the relative fraction of articles related to covid to the total number of articles per day**"""

# Tokenize the headlines and search for Covid-related words
def contains_covid_related_terms(headline):
    # Tokenize the headline into words and check if any of the covid-related words appear
    words = headline.split()
    for word in words:
        if word in covid_related_words:
            return True
    return False

# Apply previously defined cleaning function to all headlines
nyt_data_cleaned.loc[:,'cleaned_sep_headlines'] = nyt_data_cleaned['Headlines'].apply(clean_detail)

# Count how many articles per day are related to Covid
nyt_data_cleaned.loc[:,'covid_related'] = nyt_data_cleaned['cleaned_sep_headlines'].apply(contains_covid_related_terms)

nyt_covid_grouped = nyt_data_cleaned.groupby('date').agg(
    total_articles=('covid_related', 'count'),  # Total number of articles per day
    covid_articles=('covid_related', 'sum')     # Total number of Covid-related articles per day
).reset_index()

# Calculate the Covid Uncertainty Index (fraction of Covid-related articles)
nyt_covid_grouped['covid_uncertainty_index'] = nyt_covid_grouped['covid_articles'] / nyt_covid_grouped['total_articles']

# Print the result to see the Covid Uncertainty Index for each day
print(nyt_covid_grouped[['date', 'covid_uncertainty_index']].head())

# Plot the Covid Uncertainty Index over time
plt.figure(figsize=(8, 5))
plt.plot(nyt_covid_grouped['date'], nyt_covid_grouped['covid_uncertainty_index'])
plt.title('Covid Policy Uncertainty Index (EPU)', fontsize=12)
plt.xlabel('Date', fontsize=10)
plt.ylabel('Covid Index (Fraction of Covid-related articles)', fontsize=10)

# Show the plot
plt.tight_layout()
plt.show()

"""**6. Construct a daily economic policy uncertainty index.
Build the current index by estimating the relative fraction of articles that use any of those words**
"""

# Define the Economic Policy Uncertainty (EPU) related words
economic_policy_words = [
    "uncertainty", "uncertain", "economic", "economy", "congress", "deficit",
    "federal reserve", "legislation", "regulation", "white house", "uncertainties",
    "regulatory", "the fed"
]

sep_words = {"federal": "reserve", "white": "house", "the": "fed"}

# Tokenize the headlines and check for EPU-related words
def contains_epu_related_terms(headline):
    # Tokenize the headline into words and check if any of the EPU-related words appear
    words = headline.split()
    for i, word in enumerate(words):
        if word in economic_policy_words:
            return True
        if word in sep_words:
            if i + 1 < len(words) and sep_words[word] == words[i+1]:
                return True
    return False

# Count how many articles per day are related to Covid
nyt_data_cleaned.loc[:,'epu_related'] = nyt_data_cleaned['cleaned_sep_headlines'].apply(contains_epu_related_terms)

nyt_epu_grouped = nyt_data_cleaned.groupby('date').agg(
    total_articles=('epu_related', 'count'),  # Total number of articles per day
    epu_articles=('epu_related', 'sum')    # Total number of Covid-related articles per day
).reset_index()

# Calculate the Covid Uncertainty Index (fraction of Covid-related articles)
nyt_epu_grouped['epu_uncertainty_index'] = nyt_epu_grouped['epu_articles'] / nyt_epu_grouped['total_articles']

# Display the Economic Policy Uncertainty Index
print(nyt_epu_grouped[['date','epu_uncertainty_index']].head())

# Plot the Economic Policy Uncertainty Index over time
plt.figure(figsize=(8, 5))
plt.plot(nyt_epu_grouped['date'], nyt_epu_grouped['epu_uncertainty_index'])
plt.title('Economic Policy Uncertainty Index (EPU)', fontsize=12)
plt.xlabel('Date', fontsize=10)
plt.ylabel('EPU Index (Fraction of Covid-related articles)', fontsize=10)

# Show the plot
plt.tight_layout()
plt.show()

"""**8. Use the variable “Adj Close” to compute the return on S&P500 (GSPC)**"""

SP500 = pd.read_csv('drive/My Drive/Colab Notebooks/SP500.csv')
SP500.rename(columns={'Date': 'date'}, inplace=True)
SP500['date'] = pd.to_datetime(SP500['date'])
print(SP500.head())

# Sort the data by date to ensure proper calculation of returns
SP500.sort_values(by='date', ascending=True, inplace=True)
SP500['Adj Close**'] = SP500['Adj Close**'].replace({r'[,\$]': ''}, regex=True)
SP500['Adj Close**'] = pd.to_numeric(SP500['Adj Close**'])

# Calculate the daily returns using the 'Adj Close' column
SP500['Return'] = SP500['Adj Close**'].pct_change()*100

# Check the first few rows to verify the calculations
print(SP500[['date', 'Adj Close**', 'Return']].head())

results_df = pd.merge(pd.merge(nyt_covid_grouped, nyt_epu_grouped, on='date', how='inner'), SP500, on='date', how='inner')
results_df.set_index('date', inplace=True)

# Compute correlations between the three variables
correlation_matrix = results_df[['covid_uncertainty_index', 'epu_uncertainty_index', 'Return']].corr()

# Plot the correlation matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)
plt.title('Correlation Matrix between Covid Uncertainty Index, CEPI, and Returns')
plt.show()

# Plot the variables over time to visually inspect their relationships
plt.figure(figsize=(14, 8))

# Plot Covid Uncertainty Index
plt.subplot(3, 1, 1)
plt.plot(results_df.index, results_df['covid_uncertainty_index'], label='Covid Uncertainty Index', color='b')
plt.title('Covid Uncertainty Index Over Time')
plt.xlabel('Date')
plt.ylabel('Index Value')
plt.legend()

# Plot CEPI
plt.subplot(3, 1, 2)
plt.plot(results_df.index, results_df['epu_uncertainty_index'], label='CEPI', color='g')
plt.title('Coarse Economic Policy Index Over Time')
plt.xlabel('Date')
plt.ylabel('Index Value')
plt.legend()

# Plot Returns
plt.subplot(3, 1, 3)
plt.plot(results_df.index, results_df['Return'], label='Return', color='r')
plt.title('Returns Over Time')
plt.xlabel('Date')
plt.ylabel('Returns')
plt.legend()

plt.tight_layout()
plt.show()

import nltk
nltk.download('vader_lexicon')
from nltk.sentiment import SentimentIntensityAnalyzer

covid_sentiment_df = nyt_data_cleaned.copy()
covid_sentiment_df = covid_sentiment_df.reset_index()
covid_sentiment_df['date'] = pd.to_datetime(covid_sentiment_df['date'], format='%Y %b %d')

# Initialize VADER Sentiment Analyzer
analyzer = SentimentIntensityAnalyzer()

# Function to get sentiment scores (Negative, Neutral, Positive)
def get_sentiment_scores(text):
    sentiment = analyzer.polarity_scores(text)
    return sentiment['neg'], sentiment['neu'], sentiment['pos']

# Apply sentiment analysis to the filtered dataframe
covid_sentiment_df[['negative', 'neutral', 'positive']] = covid_sentiment_df['cleaned_sep_headlines'].apply(lambda x: pd.Series(get_sentiment_scores(x)))

full_date_range = pd.date_range(start=covid_sentiment_df['date'].min(), end=covid_sentiment_df['date'].max(), freq='D')
# Aggregate sentiment scores by date
daily_sentiment = covid_sentiment_df.groupby('date')[['negative', 'neutral', 'positive']].mean()
daily_sentiment = daily_sentiment.reindex(full_date_range)

covid_sentiment_covid_df = nyt_data_cleaned[nyt_data_cleaned['covid_related']]
covid_sentiment_covid_df = covid_sentiment_covid_df.reset_index()
covid_sentiment_covid_df['date'] = pd.to_datetime(covid_sentiment_covid_df['date'], format='%Y %b %d')

# Apply sentiment analysis to the filtered dataframe
covid_sentiment_covid_df[['negative', 'neutral', 'positive']] = covid_sentiment_covid_df['cleaned_sep_headlines'].apply(lambda x: pd.Series(get_sentiment_scores(x)))

# Aggregate sentiment scores by date
daily_sentiment_covid = covid_sentiment_covid_df.groupby('date')[['negative', 'neutral', 'positive']].mean()
daily_sentiment_covid = daily_sentiment_covid.reindex(full_date_range)

# Plotting the daily sentiment scores
plt.figure(figsize=(10, 6))
plt.plot(daily_sentiment_covid.index, daily_sentiment_covid['negative'], label='Negative', color='red', alpha=0.7)
plt.plot(daily_sentiment_covid.index, daily_sentiment_covid['neutral'], label='Neutral', color='gray', alpha=0.7)
plt.plot(daily_sentiment_covid.index, daily_sentiment_covid['positive'], label='Positive', color='green', alpha=0.7)

# Customizing the plot
plt.title('Daily Sentiment Analysis of Covid-Related Articles')
plt.xlabel('Date')
plt.ylabel('Sentiment Score')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()

# Show the plot
plt.show()

# Create a figure for the stacked bar plot
plt.figure(figsize=(10, 6))

# Plot the stacked bars for negative, neutral, and positive sentiment
daily_sentiment_covid[['negative', 'neutral', 'positive']].plot(kind='bar', stacked=True,
                                                                      color=['red', 'gray', 'green'],
                                                                      alpha=0.7, ax=plt.gca())

# Customizing the plot
plt.title('Normalized Daily Sentiment Analysis of Covid-Related Articles')
plt.xlabel('Date')
plt.ylabel('Normalized Sentiment Score')
plt.legend(title='Sentiment', loc='upper left', labels=['Negative', 'Neutral', 'Positive'])
plt.grid(True, axis='y')  # Show grid only on the y-axis
plt.xticks(rotation=45)
plt.gca().set_xticklabels([date.strftime('%Y-%m-%d') for date in daily_sentiment.index])
plt.tight_layout()

# Show the plot
plt.show()

# Aggregate sentiment over the entire period (mean of negative, neutral, and positive)
aggregate_sentiment = covid_sentiment_df[['negative', 'neutral', 'positive']].mean()
print(aggregate_sentiment)

# Plotting the aggregate sentiment scores as a bar plot
aggregate_sentiment.plot(kind='bar', color=['red', 'gray', 'green'], figsize=(6, 4))

# Customizing the plot
plt.title('Aggregate Sentiment for Covid-Related Articles (Entire Period)')
plt.ylabel('Average Sentiment Score')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()