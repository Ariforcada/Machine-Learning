# -*- coding: utf-8 -*-
"""Malware_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EkINCqbZ2WWSsvIAHu3TrBM01AaAlUPS
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
!pip -q install Boruta

from sklearn.inspection import permutation_importance
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, roc_auc_score, classification_report
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.linear_model import LogisticRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn import preprocessing
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

file_path = 'drive/My Drive/Colab Notebooks/Antivirus_Dataset_Jar.csv'
df = pd.read_csv(file_path, low_memory=False, sep=";")

num_columns_len = len(df.columns)
print(f"Number of columns: {num_columns_len}")

num_rows_len = len(df)
print(f"Number of rows: {num_rows_len}")
df.head()

class_col = [col for col in df.columns if 'class' in col.lower()]

x = df.drop(columns=['files', *class_col], errors='ignore')
y = df['files']
y_binary = y.apply(lambda x: 1 if '/malwares/' in x else (0 if '/benigns/' in x else None))

data_types = df.dtypes
type_counts = data_types.value_counts()
print(type_counts)
float_var = df.columns[df.dtypes == 'float64']
print(float_var)
object_var = df.columns[df.dtypes == 'object']
print(object_var)

descriptive_stats = y_binary.describe()
print(descriptive_stats)

# Set a random seed
random_seed = 72
np.random.seed(random_seed)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y_binary, test_size=0.2, random_state=random_seed)

# Pipeline for imputation and scaling
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# Fit and transform the training data, and transform the test data
x_train_scaled = pipeline.fit_transform(x_train)
x_test_scaled = pipeline.transform(x_test)

def test_model(x_train, y_train, x_test, y_test, model_type, model_opts, plot_cm=False):
    model = model_type(**model_opts)
    model.fit(x_train, y_train)
    res = dict()
    res["model"] = model
    res["score"] = model.score(x_test, y_test)

    cm = confusion_matrix(y_test, model.predict(x_test))
    disp = ConfusionMatrixDisplay(cm, display_labels=None)
    res["disp"] = disp
    if plot_cm: disp.plot()

    res["df"] = pd.DataFrame(
        cm,
        index=["Actual Negative", "Actual Positive"],
        columns=["Predicted Negative", "Predicted Positive"]
    )

    res["false_pos_rate"] = cm[0, 1]/cm[0].sum()
    res["recall_rate"] = cm[1, 1]/cm[1].sum()
    res["accuracy_rate"] = accuracy_score(y_test, model.predict(x_test))
    y_pred_prob = model.predict_proba(x_test)[:, 1]
    res["AUC"] = roc_auc_score(y_test, y_pred_prob)
    return res

models = [
    *[(KNeighborsClassifier, {"n_neighbors": n}) for n in [1, 3, 5,9,10,11,20]],
    (LogisticRegression, {}),
    (DecisionTreeClassifier, {}),
    (RandomForestClassifier, {}),
    (LinearDiscriminantAnalysis, {}),
    (GaussianNB, {})
]


# Save the scores, false positive rates, and recall rates for all models:
comp_res = {
    "Name": list(),
    "False Positive Rate": list(),
    "Recall Rate": list(),
    "Accuracy Rate": list(),
    "AUC": list()
}

trained_models = dict()
for model, model_opts in models:

    res = test_model(x_train_scaled, y_train, x_test_scaled, y_test,
                     model, model_opts)

    comp_res["Name"].append(model.__name__ + '; '.join([str(v) for v in model_opts.values()]))
    comp_res["False Positive Rate"].append(res["false_pos_rate"])
    comp_res["Recall Rate"].append(res["recall_rate"])
    comp_res["Accuracy Rate"].append(res["accuracy_rate"])
    comp_res["AUC"].append(res["AUC"])
    trained_models[comp_res["Name"][-1]] = res["model"]

"""FIT ON ENTIRE DATASET"""

#Show a Dataframe of the Results dictionary
pd.DataFrame(comp_res).set_index("Name")

results = {}

# Loop over models and store the results in the dictionary
for model, model_opts in models:
    # Call test_model for each model and store results
    model_name = model.__name__ + '; ' + ', '.join([f'{k}={v}' for k, v in model_opts.items()])
    results[model_name] = test_model(x_train_scaled, y_train, x_test_scaled, y_test, model, model_opts, plot_cm=True)

# Get feature importances from the random forest model
rf_model = trained_models['RandomForestClassifier']
importances = rf_model.feature_importances_
feature_names = x.columns.tolist()
indices = np.argsort(importances)[::-1]

indices_20 = indices[:20]

# Set up the plot
plt.figure(figsize=(10, 8))
plt.title("Feature Importances from Random Forest Model")
plt.bar(range(20), importances[indices_20], color="skyblue", align="center")
plt.xticks(range(20), [feature_names[i] for i in indices_20], rotation=90)
plt.xlim([-1, 20])
plt.ylabel("Importance")
plt.xlabel("Features")
plt.tight_layout()
plt.show()

print(trained_models)

# Get feature importances from the random forest model
tree_model = trained_models['DecisionTreeClassifier']
importances = tree_model.feature_importances_
feature_names = x.columns.tolist()
indices = np.argsort(importances)[::-1]

indices_20 = indices[:20]

# Set up the plot
plt.figure(figsize=(10, 2))
plt.title("Feature Importances from Decision Tree Model")
plt.bar(range(20), importances[indices_20], color="skyblue", align="center")
plt.xticks(range(20), [feature_names[i] for i in indices_20], rotation=90)
plt.xlim([-1, 20])
plt.ylabel("Importance")
plt.xlabel("Features")
plt.tight_layout()
plt.show()

log_model = trained_models['LogisticRegression']
# Extract the feature coefficients (importance)
importances = log_model.coef_[0]  # We take the first row in case of multiple classes (binary classification)
feature_names = x.columns.tolist()

# Sort features by absolute importance (coefficients)
indices = np.argsort(np.abs(importances))[::-1]

# Select the top 20 most important features
indices_20 = indices[:20]
top_20_features = [feature_names[i] for i in indices_20]
top_20_importances = importances[indices_20]

plt.figure(figsize=(10, 2))
plt.bar(top_20_features, top_20_importances, color='skyblue')
plt.xticks(range(20), top_20_features, rotation=90)
plt.xlabel('Coefficient Value')
plt.title('Top 20 Most Important Features (Logistic Regression)')
plt.show()

lda_model = trained_models['LinearDiscriminantAnalysis']
# Extract the feature coefficients (importance)
importances = lda_model.coef_[0]  # We take the first row in case of multiple classes (binary classification)
feature_names = x.columns.tolist()

# Sort features by absolute importance (coefficients)
indices = np.argsort(np.abs(importances))[::-1]

# Select the top 20 most important features
indices_20 = indices[:20]
top_20_features = [feature_names[i] for i in indices_20]
top_20_importances = importances[indices_20]

plt.figure(figsize=(10, 6))
plt.bar(top_20_features, top_20_importances, color='skyblue')
plt.xticks(range(20), top_20_features, rotation=90)
plt.xlabel('Coefficient Value')
plt.title('Top 20 Most Important Features (Linear Discriminant Analysis Regression)')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt


# Create a DataFrame with the top 20 features
top_20_df = x.iloc[:, indices_20]
# Calculate the correlation matrix for the top 20 features
correlation_matrix_top_20 = top_20_df.corr()

# Set up the matplotlib figure
plt.figure(figsize=(12, 8))

# Draw the heatmap
sns.heatmap(correlation_matrix_top_20, annot=True, fmt=".2f", cmap='coolwarm', square=True, cbar_kws={"shrink": .8})
plt.title("Heatmap of Top 20 Important Features")
plt.show()

# Calculate the correlation matrix for all features
correlation_matrix_all = x.corr()

# Initialize a list to hold features to drop
high_corr_vars = set()  # Use a set to avoid duplicates

# Loop through the correlation matrix to find highly correlated pairs
for i in range(len(correlation_matrix_all.columns)):
    for j in range(i + 1, len(correlation_matrix_all.columns)):
        if abs(correlation_matrix_all.iloc[i, j])>= 0.95:  # You can adjust the threshold
            high_corr_vars.add(correlation_matrix_all.columns[i])  # Add the feature to the set
            high_corr_vars.add(correlation_matrix_all.columns[j])  # Add the other feature to the set

# Drop the highly correlated features from the dataset
x_clean = x.drop(columns=high_corr_vars, errors='ignore')

x_clean_df = pd.DataFrame(x_clean)
print(x_clean_df.shape)
data_types_clean = x_clean_df.dtypes
type_counts_clean = data_types_clean.value_counts()
print(type_counts_clean)

""">0.95 --> 1588 predictors
== 1 --> 3165.
"""

y_binary_clean = y_binary.drop(columns=high_corr_vars, errors='ignore')
print(f"y_train shape: {y_binary_clean.shape}")

# Set a random seed
random_seed = 72
np.random.seed(random_seed)

# Split the data into training and testing sets
x_train_clean, x_test_clean, y_train_clean, y_test_clean = train_test_split(x_clean_df, y_binary_clean, test_size=0.2, random_state=random_seed)

# Pipeline for imputation and scaling
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# Fit and transform the training data, and transform the test data
x_train_scaled_clean = pipeline.fit_transform(x_train_clean)
x_test_scaled_clean = pipeline.transform(x_test_clean)

def test_model(x_train_clean, y_train_clean, x_test_clean, y_test_clean, model_type, model_opts, plot_cm=False):
    model = model_type(**model_opts)
    model.fit(x_train_clean, y_train_clean)
    res = dict()
    res["model"] = model
    res["score"] = model.score(x_test_clean, y_test_clean)

    cm = confusion_matrix(y_test_clean, model.predict(x_test_clean))
    disp = ConfusionMatrixDisplay(cm, display_labels=None)
    res["disp"] = disp
    if plot_cm: disp.plot()

    res["df"] = pd.DataFrame(
        cm,
        index=["Actual Negative", "Actual Positive"],
        columns=["Predicted Negative", "Predicted Positive"]
    )

    res["false_pos_rate"] = cm[0, 1]/cm[0].sum()
    res["recall_rate"] = cm[1, 1]/cm[1].sum()
    res["accuracy_rate"] = accuracy_score(y_test_clean, model.predict(x_test_clean))
    y_pred_prob = model.predict_proba(x_test_clean)[:, 1]
    res["AUC"] = roc_auc_score(y_test_clean, y_pred_prob)
    return res

models = [
    *[(KNeighborsClassifier, {"n_neighbors": n}) for n in [1, 3, 5,9,10,11,20]],
    (LogisticRegression, {}),
    (DecisionTreeClassifier, {}),
    (RandomForestClassifier, {}),
    (LinearDiscriminantAnalysis, {}),
    (GaussianNB, {})
]


# Save the scores, false positive rates, and recall rates for all models:
comp_res = {
    "Name": list(),
    "False Positive Rate": list(),
    "Recall Rate": list(),
    "Accuracy Rate": list(),
    "AUC": list()
}

trained_models = dict()
for model, model_opts in models:

    res = test_model(x_train_scaled_clean, y_train_clean, x_test_scaled_clean, y_test_clean,
                     model, model_opts)

    comp_res["Name"].append(model.__name__ + '; '.join([str(v) for v in model_opts.values()]))
    comp_res["False Positive Rate"].append(res["false_pos_rate"])
    comp_res["Recall Rate"].append(res["recall_rate"])
    comp_res["Accuracy Rate"].append(res["accuracy_rate"])
    comp_res["AUC"].append(res["AUC"])
    trained_models[comp_res["Name"][-1]] = res["model"]

"""FIT ON 1588 FEATURES AFTER CORR>0.95 EXTRACTION"""

#Show a Dataframe of the Results dictionary
pd.DataFrame(comp_res).set_index("Name")

print(x_train_scaled_clean.shape)
x_test_scaled_clean.shape

results = {}

# Loop over models and store the results in the dictionary
for model, model_opts in models:
    # Call test_model for each model and store results
    model_name = model.__name__ + '; ' + ', '.join([f'{k}={v}' for k, v in model_opts.items()])
    results[model_name] = test_model(x_train_scaled_clean, y_train_clean, x_test_scaled_clean, y_test_clean, model, model_opts, plot_cm=True)

#boruta

from boruta import BorutaPy
import numpy as np

x_train_scaled_clean = pd.DataFrame(x_train_scaled_clean)
x_test_scaled_clean = pd.DataFrame(x_test_scaled_clean)
# Initialize Boruta
boruta = BorutaPy(
    estimator=RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5),
    n_estimators='auto',
    verbose=2,
    random_state=42
)

# Fit Boruta
boruta.fit(x_train_scaled_clean, y_train_clean)
confirmed_features = x_train_scaled_clean.columns[boruta.support_].tolist()
print("Confirmed features:", confirmed_features)

from boruta import BorutaPy
import numpy as np

x_train_scaled_clean = pd.DataFrame(x_train_scaled_clean)
# Initialize Boruta
boruta = BorutaPy(
    estimator=RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=10),
    n_estimators='auto',
    verbose=2,
    random_state=42
)

# Fit Boruta
boruta.fit(x_train_scaled_clean, y_train_clean)
confirmed_features_10 = x_train_scaled_clean.columns[boruta.support_].tolist()
print("Confirmed features:", confirmed_features_10)

"""# APPY BORUTA FEATURE SELECTION ON THE RANDOM FOREST --> 73 FEATURES"""

x_train_scaled_clean = pd.DataFrame(x_train_scaled_clean)
# Initialize Boruta
boruta = BorutaPy(
    estimator=RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5),
    n_estimators=1000,
    verbose=2,
    random_state=42
)

# Fit Boruta
boruta.fit(x_train_scaled_clean, y_train_clean)
confirmed_features_n = x_train_scaled_clean.columns[boruta.support_].tolist()
print("Confirmed features:", confirmed_features_n)

# filter training data to keep only the confirmed features
x_train_scaled_clean = pd.DataFrame(x_train_scaled_clean)
x_test_scaled_clean = pd.DataFrame(x_test_scaled_clean)
y_test_clean = pd.DataFrame(y_test_clean)

x_boruta_train = x_train_scaled_clean[confirmed_features_n]
x_boruta_test = x_test_scaled_clean[confirmed_features_n]

# Verify the number of selected features
print(len(confirmed_features_n))

print(x_boruta_train.shape)
print(x_boruta_test.shape)
print(y_train_clean.shape)
print(y_test_clean.shape)

models = [
    (LinearDiscriminantAnalysis, {}),
    (GaussianNB, {}),
    (KNeighborsClassifier, {"n_neighbors": 9, "metric": "euclidean", "weights": "distance"}),  # Fine-tuned KNN
    (LogisticRegression, {"C": 10, "penalty": "l1", "solver": "liblinear"}),  # Fine-tuned Logistic Regression with solver='liblinear'
    (DecisionTreeClassifier, {"max_depth": 20, "max_features": "sqrt", "min_samples_leaf": 2, "min_samples_split": 5}),  # Fine-tuned Decision Tree
    (RandomForestClassifier, {"max_depth": 15, "max_features": "sqrt", "min_samples_leaf": 1, "min_samples_split": 2, "n_estimators": 300}),  # Fine-tuned Random Forest

]


# Save the scores, false positive rates, and recall rates for all models:
comp_res = {
    "Name": list(),
    "False Positive Rate": list(),
    "Recall Rate": list(),
    "Accuracy Rate": list(),
    "AUC": list()
}

trained_models = dict()
for model, model_opts in models:

    res = test_model(x_boruta_train, y_train_clean, x_boruta_test, y_test_clean,
                     model, model_opts)

    comp_res["Name"].append(model.__name__ + '; '.join([str(v) for v in model_opts.values()]))
    comp_res["False Positive Rate"].append(res["false_pos_rate"])
    comp_res["Recall Rate"].append(res["recall_rate"])
    comp_res["Accuracy Rate"].append(res["accuracy_rate"])
    comp_res["AUC"].append(res["AUC"])
    trained_models[comp_res["Name"][-1]] = res["model"]

#Show a Dataframe of the Results dictionary
pd.DataFrame(comp_res).set_index("Name")

# Define a function to plot a bar graph

df = pd.DataFrame(comp_res)


# Define a function to map the full model names to shorter versions in the plot
def shorten_model_name(model_name):
    if "KNeighborsClassifier" in model_name:
        return "KNN " + model_name.split("n_neighbors=")[-1].split(",")[0]
    elif "LogisticRegression10" in model_name:
        return "LogReg " + model_name.split("C=")[-1].split(",")[0]
    elif "DecisionTreeClassifier" in model_name:
        return "DT " + model_name.split("max_depth=")[-1].split(",")[0]
    elif "RandomForestClassifier" in model_name:
        return "RF " + model_name.split("max_depth=")[-1].split(",")[0]
    elif "LinearDiscriminantAnalysis" in model_name:
        return "LDA"
    elif "GaussianNB" in model_name:
        return "GNB"
    else:
        return model_name  # Default case for unknown models

# Apply the shortening function to the 'Name' column
df["Short Name"] = df["Name"].apply(shorten_model_name)

# Set "Short Name" column as the index
df.set_index("Short Name", inplace=True)

# Set "Name" column as the index
df.set_index("Name", inplace=True)

def plot_metric(df, metric_name, title):
    plt.figure(figsize=(10, 6))
    sns.barplot(x=df.index, y=metric_name, data=df, palette="viridis")
    plt.title(title)
    plt.xlabel("Models")
    plt.ylabel(metric_name)
    plt.xticks(rotation=90)
    plt.show()

# Plot Accuracy
plot_metric(df, "Accuracy Rate", "Model Comparison - Accuracy")

# Plot AUC
plot_metric(df, "AUC", "Model Comparison - AUC")

# Plot False Positive Rate
plot_metric(df, "False Positive Rate", "Model Comparison - False Positive Rate")

# Plot Recall Rate
plot_metric(df, "Recall Rate", "Model Comparison - Recall Rate")

models = [
    (KNeighborsClassifier, {"n_neighbors": 9, "metric": "euclidean", "weights": "distance"}),  # Fine-tuned KNN
    (LogisticRegression, {"C": 10, "penalty": "l1", "solver": "liblinear"}),  # Fine-tuned Logistic Regression with solver='liblinear'
    (DecisionTreeClassifier, {"max_depth": 20, "max_features": "sqrt", "min_samples_leaf": 2, "min_samples_split": 5}),  # Fine-tuned Decision Tree
    (RandomForestClassifier, {"max_depth": 15, "max_features": "sqrt", "min_samples_leaf": 1, "min_samples_split": 2, "n_estimators": 300}),  # Fine-tuned Random Forest
    (LinearDiscriminantAnalysis, {"shrinkage": None, "solver": "svd"}),  # Fine-tuned LDA
    (GaussianNB, {"var_smoothing": 1e-09})  # Fine-tuned GaussianNB
]

results = {}

# Loop over models and store the results in the dictionary
for model, model_opts in models:
    # Call test_model for each model and store results
    model_name = model.__name__ + '; ' + ', '.join([f'{k}={v}' for k, v in model_opts.items()])
    results[model_name] = test_model(x_boruta_train, y_train_clean, x_boruta_test, y_test_clean, model, model_opts, plot_cm=True)



from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))

for model_name, result in results.items():
    y_pred_prob = result['model'].predict_proba(x_boruta_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test_clean, y_pred_prob)
    roc_auc = auc(fpr, tpr)
    short_model_name = model_name.split(';')[0]  # Extract just the model name (before ';')
    plt.plot(fpr, tpr, label=f'{short_model_name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', linestyle='--')  # Random classifier line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""FINE TUNE KNN
Best parameters for KNN: {'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'distance'}
Best cross-validation accuracy: 0.972946151082635
Test set accuracy: 0.8725
"""

#FINE TUNE KNN  --> roc auc criteria

from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

# Define the hyperparameter grid for KNN
param_grid = {
    'n_neighbors': [1, 3, 5, 7, 9, 10, 11],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

# Initialize the KNN model
knn = KNeighborsClassifier()

# Perform grid search for hyperparameter tuning
#y_train_clean = y_train_clean.iloc[:, 0].ravel()  # Select the first column and then ravel it
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='roc_auc', verbose=2)
grid_search.fit(x_boruta_train, y_train_clean)

# Print the best parameters and the corresponding score
print(f"Best parameters for KNN: {grid_search.best_params_}")
print(f"Best cross-validation accuracy: {grid_search.best_score_}")

best_knn = grid_search.best_estimator_
test_accuracy = best_knn.score(x_boruta_test, y_test_clean)  # Use your test set
print(f"Test set accuracy: {test_accuracy}")

"""FINE TUNE RANDOM FOREST
Best parameters for Random Forest: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}
Best score for Random Forest: 0.9724353448275862
"""

#TUNING RANDOM FOREST --> CV 5

# Define the hyperparameter grid for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['auto', 'sqrt', 'log2']
}

# Initialize the Random Forest model
rf = RandomForestClassifier()

# Perform GridSearchCV
grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_rf.fit(x_boruta_train, y_train_clean)

# Print the best hyperparameters and score
print("Best parameters for Random Forest:", grid_search_rf.best_params_)
print("Best score for Random Forest:", grid_search_rf.best_score_)

"""Best parameters for Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}
Best score for Random Forest: 0.9736910377358491
"""

#TUNING RANDOM FOREST  --> CV 10

# Define the hyperparameter grid for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['auto', 'sqrt', 'log2']
}

# Initialize the Random Forest model
rf = RandomForestClassifier()

# Perform GridSearchCV
grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=10, scoring='accuracy', n_jobs=-1)
grid_search_rf.fit(x_boruta_train, y_train_clean)

# Print the best hyperparameters and score
print("Best parameters for Random Forest:", grid_search_rf.best_params_)
print("Best score for Random Forest:", grid_search_rf.best_score_)

"""FINE TUNE LOGISTIC

Best Parameters: {'C': 10, 'penalty': 'l1'}
Best Cross-Validation Score: 0.9298334639498431
Test Accuracy: 0.9375
"""

#FINE TUNE LOGISTIC

# Define the parameter grid
param_grid = {
    'penalty': ['l1', 'l2'],
    'C': [0.1, 1, 10, 100]
}

# Set a random seed for reproducibility
np.random.seed(random_seed)

#logistic_model = trained_models['LogisticRegression']
logistic_model = LogisticRegression(solver='liblinear', random_state=random_seed)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=logistic_model, param_grid=param_grid,
                           scoring='accuracy', cv=5, verbose=2)

# Fit the model
grid_search.fit(x_boruta_train, y_train_clean)

# Best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best Cross-Validation Score:", grid_search.best_score_)

# Predict on the test set using the best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(x_boruta_test)

# Evaluate the model
print("Test Accuracy:", accuracy_score(y_test_clean, y_pred))
print(classification_report(y_test_clean, y_pred))

"""FINE TUNE DECISION TREE
Best parameters for Decision Tree: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}
Best score for Decision Tree: 0.919194749216301
"""

#FINE TUNE DECISION TREE

# Define the hyperparameter grid for Decision Tree
param_grid_tree = {
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['auto', 'sqrt', 'log2']
}

# Initialize the Decision Tree model
dt = DecisionTreeClassifier()

# Perform GridSearchCV
grid_search_tree = GridSearchCV(estimator=dt, param_grid=param_grid_tree, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_tree.fit(x_boruta_train, y_train_clean)

# Print the best hyperparameters and score
print("Best parameters for Decision Tree:", grid_search_tree.best_params_)
print("Best score for Decision Tree:", grid_search_tree.best_score_)

"""TUNING LDA

Best parameters for LDA: {'shrinkage': None, 'solver': 'svd'}
Best score for LDA: 0.87782131661442
"""

#TUNING LDA

# Define the hyperparameter grid for LDA
param_grid_lda = {
    'solver': ['svd', 'lsqr', 'eigen'],
    'shrinkage': ['auto', None]  # Only relevant for 'lsqr'
}

# Initialize the LDA model
lda = LinearDiscriminantAnalysis()

# Perform GridSearchCV
grid_search_lda = GridSearchCV(estimator=lda, param_grid=param_grid_lda, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_lda.fit(x_boruta_train, y_train_clean)

# Print the best hyperparameters and score
print("Best parameters for LDA:", grid_search_lda.best_params_)
print("Best score for LDA:", grid_search_lda.best_score_)

"""TUNING GAUSSIAN

est parameters for GaussianNB: {'var_smoothing': 1e-09}
Best score for GaussianNB: 0.7888636363636363
"""

#TUNING GAUSSIAN

# Define the hyperparameter grid for GaussianNB
param_grid_nb = {
    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]
}

# Initialize the GaussianNB model
nb = GaussianNB()

# Perform GridSearchCV
grid_search_nb = GridSearchCV(estimator=nb, param_grid=param_grid_nb, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_nb.fit(x_boruta_train, y_train_clean)

# Print the best hyperparameters and score
print("Best parameters for GaussianNB:", grid_search_nb.best_params_)
print("Best score for GaussianNB:", grid_search_nb.best_score_)

"""FIT MODELS WITH THE FINE TUNED PARAMETERS"""

# Define the models with their fine-tuned parameters
models = [
    (KNeighborsClassifier, {"n_neighbors": 9, "metric": "euclidean", "weights": "distance"}),  # Fine-tuned KNN
    (LogisticRegression, {"C": 10, "penalty": "l1", "solver": "liblinear"}),  # Fine-tuned Logistic Regression with solver='liblinear'
    (DecisionTreeClassifier, {"max_depth": 20, "max_features": "sqrt", "min_samples_leaf": 2, "min_samples_split": 5}),  # Fine-tuned Decision Tree
    (RandomForestClassifier, {"max_depth": 15, "max_features": "sqrt", "min_samples_leaf": 1, "min_samples_split": 2, "n_estimators": 300}),  # Fine-tuned Random Forest
    (LinearDiscriminantAnalysis, {"shrinkage": None, "solver": "svd"}),  # Fine-tuned LDA
    (GaussianNB, {"var_smoothing": 1e-09})  # Fine-tuned GaussianNB
]

# Save the scores, false positive rates, and recall rates for all models:
comp_res = {
    "Name": list(),
    "False Positive Rate": list(),
    "Recall Rate": list(),
    "Accuracy Rate": list(),
    "AUC": list()
}

trained_models = dict()
for model, model_opts in models:
    # Test each model using the fine-tuned parameters
    res = test_model(x_boruta_train, y_train_clean, x_boruta_test, y_test_clean,
                     model, model_opts)

    # Store the results for each model
    comp_res["Name"].append(model.__name__ + '; '.join([f"{key}={value}" for key, value in model_opts.items()]))
    comp_res["False Positive Rate"].append(res["false_pos_rate"])
    comp_res["Recall Rate"].append(res["recall_rate"])
    comp_res["Accuracy Rate"].append(res["accuracy_rate"])
    comp_res["AUC"].append(res["AUC"])

    # Save the trained model in a dictionary with its name as the key
    trained_models[comp_res["Name"][-1]] = res["model"]

comp_res_df = pd.DataFrame(comp_res)

#Show a Dataframe of the Results dictionary
pd.DataFrame(comp_res).set_index("Name")

"""LASSO AND RIDGE"""

# Apply Lasso regression
lasso = Lasso(alpha=0.01)
lasso.fit(x_train_scaled_clean, y_train_clean)

lasso_coefficients = lasso.coef_
lasso_selected_features = x_train_scaled_clean.columns[lasso_coefficients != 0]
x_train_lasso_selected = x_train_scaled_clean[lasso_selected_features]
x_test_lasso_selected = x_test_scaled_clean[lasso_selected_features]

x_train_lasso_selected = pd.DataFrame(x_train_lasso_selected)
x_test_lasso_selected = pd.DataFrame(x_test_lasso_selected)

# Apply Ridge regression
ridge = Ridge(alpha=1.0)
ridge.fit(x_train_scaled_clean, y_train_clean)

ridge_coefficients = ridge.coef_
ridge_selected_features = x_train_scaled_clean.columns[np.abs(ridge_coefficients) > 0]
x_train_ridge_selected = x_train_scaled_clean[ridge_selected_features]
x_test_ridge_selected = x_test_scaled_clean[ridge_selected_features]

x_train_ridge_selected = pd.DataFrame(x_train_ridge_selected)
x_test_ridge_selected = pd.DataFrame(x_test_ridge_selected)

# Save the scores, false positive rates, and recall rates for all models:
comp_res = {
    "Name": list(),
    "False Positive Rate": list(),
    "Recall Rate": list(),
    "Accuracy Rate": list(),
    "AUC": list()
}

trained_models = dict()
for model, model_opts in models:

    res = test_model(x_train_lasso_selected, y_train_clean, x_test_lasso_selected, y_test_clean,
                     model, model_opts)

    comp_res["Name"].append(model.__name__ + '; '.join([str(v) for v in model_opts.values()]))
    comp_res["False Positive Rate"].append(res["false_pos_rate"])
    comp_res["Recall Rate"].append(res["recall_rate"])
    comp_res["Accuracy Rate"].append(res["accuracy_rate"])
    comp_res["AUC"].append(res["AUC"])
    trained_models[comp_res["Name"][-1]] = res["model"]

#Show a Dataframe of the Results dictionary
pd.DataFrame(comp_res).set_index("Name")

# Save the scores, false positive rates, and recall rates for all models:
comp_res = {
    "Name": list(),
    "False Positive Rate": list(),
    "Recall Rate": list(),
    "Accuracy Rate": list(),
    "AUC": list()
}

trained_models = dict()
for model, model_opts in models:

    res = test_model(x_train_ridge_selected, y_train_clean, x_test_ridge_selected, y_test_clean,
                     model, model_opts)

    comp_res["Name"].append(model.__name__ + '; '.join([str(v) for v in model_opts.values()]))
    comp_res["False Positive Rate"].append(res["false_pos_rate"])
    comp_res["Recall Rate"].append(res["recall_rate"])
    comp_res["Accuracy Rate"].append(res["accuracy_rate"])
    comp_res["AUC"].append(res["AUC"])
    trained_models[comp_res["Name"][-1]] = res["model"]

#Show a Dataframe of the Results dictionary
pd.DataFrame(comp_res).set_index("Name")

#FORWARD FEATURE SELECTION USING RANDOM FOREST

from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.preprocessing import StandardScaler

rf_model = RandomForestRegressor()

# Perform forward feature selection using RandomForestRegressor
sfs_rf = SequentialFeatureSelector(rf_model, n_features_to_select='auto', direction='forward', cv=5)
sfs_rf.fit(x_train_scaled_clean, y_train_clean)

# Get the selected features
selected_features_rf = x_train_scaled_clean.columns[sfs_rf.get_support()]
x_train_selected_rf = x_train_scaled_clean[selected_features_rf]
x_test_selected_rf = x_test_scaled_clean[selected_features_rf]



