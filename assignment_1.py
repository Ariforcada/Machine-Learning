# -*- coding: utf-8 -*-
"""Assignment 1 - Ariadna Forcada.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cE1teXquZ_mlyAG58eXglOiq5ehS31Vh
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

from sklearn.metrics import (
    confusion_matrix,
    ConfusionMatrixDisplay,
    accuracy_score,
    roc_auc_score,
    classification_report,
    roc_curve,
    precision_recall_curve,
    average_precision_score,
)
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn import preprocessing
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

"""This script has been adapted to run outside Colab.
It now:
- Loads `mobile_money.csv` from common local paths if available
- Adds several EDA and model evaluation plots
- Fixes the hyperparameter (C) plot tick labels
"""

"""**1. READ FILE**

Reads `mobile_money.csv` from a local path. You can place the file in the
repository root or pass a custom path via the `MOBILE_MONEY_PATH` env var.
"""

def load_mobile_money_dataframe() -> pd.DataFrame:
    possible_paths = [
        Path("mobile_money.csv"),
        Path("/workspace/mobile_money.csv"),
        Path("./data/mobile_money.csv"),
        # Legacy Colab path for reference if running in that environment
        Path("drive/My Drive/Colab Notebooks//mobile_money.csv"),
    ]
    env_path = Path(str(Path.cwd() / Path(
        (os.environ.get("MOBILE_MONEY_PATH") or "").strip()
    ))) if 'MOBILE_MONEY_PATH' in os.environ and os.environ.get("MOBILE_MONEY_PATH") else None

    candidates = [p for p in ([env_path] if env_path else []) + possible_paths if p and p.exists()]
    if not candidates:
        raise FileNotFoundError(
            "mobile_money.csv not found. Place it in the repo root or set MOBILE_MONEY_PATH."
        )
    return pd.read_csv(candidates[0])


import os  # placed after function to avoid unused if module imported

mobile_money_df = load_mobile_money_dataframe()

"""**2. DESCRIPTIVE STATS 'mpesa user'**"""

descriptive_stats = mobile_money_df['mpesa_user'].describe()
print(descriptive_stats)

"""**3. DESCRIPTIVE STATS VARIABLES OF INTEREST DEPENDING ON 'mpesa user'**"""

variables_of_interest = [
    'cellphone', #cellphone
    'totexppc', #per capita consumption
    'wkexppc', #per capita food consumption
    'wealth', #total wealth
    'size', #household size
    'education_years', #education head years
    'pos', #positive shock
    'neg', #negative shock
    'ag',  #weather agricultural shock
    'sick', #illness shock
    'sendd', #send remittances
    'recdd', #receive remittances
    'bank_acct', #bank account
    'mattress', #mattress
    'sacco', #sacco
    'merry', #merry go round
    'occ_farmer', #farmer
    'occ_public', #public service
    'occ_prof', #professional occupation
    'occ_help', #househelp
    'occ_bus', #run business
    'occ_sales', #in sales
    'occ_ind', #in industry
    'occ_other', #other occupation
    'occ_ue', #unemployed
]

descriptive_stats1 = mobile_money_df.groupby('mpesa_user')[variables_of_interest].describe()
print(descriptive_stats1)

"""Additional EDA: class balance, distributions, and correlations"""

# Ensure plotting style
sns.set(style="whitegrid")

# 3.A Class balance
plt.figure(figsize=(5, 4))
mobile_money_df['mpesa_user'].value_counts(dropna=False).sort_index().plot(
    kind='bar', color=['steelblue', 'orange']
)
plt.title('Class Balance: mpesa_user')
plt.xlabel('mpesa_user')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

# 3.B Numeric distributions by class (key variables)
numeric_for_dist = [
    'totexppc',
    'wkexppc',
    'wealth',
    'size',
    'education_years',
]

for col in [c for c in numeric_for_dist if c in mobile_money_df.columns]:
    plt.figure(figsize=(6, 4))
    sns.kdeplot(
        data=mobile_money_df.dropna(subset=[col, 'mpesa_user']),
        x=col,
        hue='mpesa_user',
        common_norm=False,
        fill=True,
        alpha=0.4,
    )
    plt.title(f'Distribution by mpesa_user: {col}')
    plt.tight_layout()
    plt.show()

# 3.C Correlation heatmap for numeric variables of interest
numeric_cols = [c for c in variables_of_interest if c in mobile_money_df.columns and pd.api.types.is_numeric_dtype(mobile_money_df[c])]
if numeric_cols:
    plt.figure(figsize=(10, 8))
    corr = mobile_money_df[numeric_cols].corr()
    sns.heatmap(corr, cmap='coolwarm', center=0, square=False)
    plt.title('Correlation Heatmap (variables_of_interest)')
    plt.tight_layout()
    plt.show()

"""**4. CONSTRUCT THE LOGISTIC CLASSIFIER, RANDOM FOREST CLASSIFIER, LINEAR DISCRIMINANT ANALYSIS**

4.1. In order to do this, we will first divide the data between training and test data, encode the categorical predictors and standardize the data. In order to construct the classifiers, the variables can't have any NaN values, so the process of standardizing has been executed through a pipeline that performs two functions:
  - Imputation (SimpleImputer): It fills in any missing values in the dataset with the mean of the respective feature.
  - Scaling (StandardScaler): It standardizes the data by transforming each feature to have a mean of 0 and a standard deviation of 1.

4.2. Construct the classifiers using an adapted version of the function shown in the tutorial sessions. A random seed is set to ensure reproducibility. The function takes as inputs x_train, y_train, x_test, and y_test, the model_type (LogisticRegression, RandomForestClassifier, and Linear Analysis). It fits the training data according to each specified model and computes the false positive rate, recall rate, accuracy, and AUC for each.

Note: Another way to calculate accuracy would be to use the "score" instruction as follows, which yields the same result as how it has been calculated in our main code.
res["score"] = model.score(x_test, y_test)
comp_res["Score"].append(res["score"])
"""

#4.1. Set up data

# Independent and Dependent variables
x = mobile_money_df[variables_of_interest]
y = mobile_money_df['mpesa_user']
x_encoded = pd.get_dummies(x, drop_first=True)

# Set a random seed
random_seed = 72
np.random.seed(random_seed)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x_encoded, y, test_size=0.2, random_state=random_seed)

# Pipeline for imputation and scaling
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# Fit and transform the training data, and transform the test data
x_train_scaled = pipeline.fit_transform(x_train)
x_test_scaled = pipeline.transform(x_test)

#4.2 Construct classifiers

def test_model(x_train, y_train, x_test, y_test, model_type, model_opts, plot_cm=False):
    model = model_type(**model_opts)
    model.fit(x_train, y_train)
    res = dict()
    res["model"] = model
    res["score"] = model.score(x_test, y_test)

    y_pred = model.predict(x_test)
    cm = confusion_matrix(y_test, y_pred)
    # Use sorted unique labels for display
    labels = np.unique(np.concatenate([np.asarray(y_test), y_pred]))
    disp = ConfusionMatrixDisplay(cm, display_labels=labels)
    res["disp"] = disp
    if plot_cm: disp.plot()

    res["df"] = pd.DataFrame(
        cm,
        index=["Actual Negative", "Actual Positive"],
        columns=["Predicted Negative", "Predicted Positive"]
    )

    res["false_pos_rate"] = cm[0, 1]/cm[0].sum()
    res["recall_rate"] = cm[1, 1]/cm[1].sum()
    res["accuracy_rate"] = accuracy_score(y_test, y_pred)
    y_pred_prob = model.predict_proba(x_test)[:, 1]
    res["AUC"] = roc_auc_score(y_test, y_pred_prob)
    # Add ROC/PR computed arrays for downstream plotting
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)
    res["roc_curve"] = (fpr, tpr)
    res["pr_curve"] = (precision, recall)
    res["average_precision"] = average_precision_score(y_test, y_pred_prob)
    return res

models = [
    (LogisticRegression, {}),
    (RandomForestClassifier, {}),
    (LinearDiscriminantAnalysis, {})
]

# Save the scores, false positive rates, and recall rates for all models:
comp_res = {
    "Name": list(),
    "False Positive Rate": list(),
    "Recall Rate": list(),
    "Accuracy Rate": list(),
    "AUC": list()
}

trained_models = dict()
model_results = dict()
for model, model_opts in models:
    res = test_model(x_train_scaled, y_train, x_test_scaled, y_test, model, model_opts)
    name = model.__name__ + (' ' + ', '.join([f'{k}={v}' for k, v in model_opts.items()]) if model_opts else '')
    comp_res["Name"].append(name if name.strip() else model.__name__)
    comp_res["False Positive Rate"].append(res["false_pos_rate"])
    comp_res["Recall Rate"].append(res["recall_rate"])
    comp_res["Accuracy Rate"].append(res["accuracy_rate"])
    comp_res["AUC"].append(res["AUC"])
    trained_models[model.__name__] = res["model"]
    model_results[model.__name__] = res

"""**5. ACCURACY RATE AND AUC CRITERIA**

5.1. With the results provided in the Results dictionary, we can see that both Logistic Regression and Random Forest achieved perfect scores in both accuracy and AUC, so either of these classifiers can be considered the best based on these criteria.

5.2. Extra: Confusion matrices of the three criteria.
"""

#5.1. Show a DataFrame of the Results dictionary and plot comparison bars

comp_df = pd.DataFrame(comp_res).set_index("Name")
print(comp_df)

plt.figure(figsize=(9, 5))
comp_df[["Accuracy Rate", "AUC", "Recall Rate", "False Positive Rate"]].plot(
    kind="bar", figsize=(11, 6), colormap="viridis"
)
plt.title("Model Comparison")
plt.ylabel("Score")
plt.tight_layout()
plt.show()

#5.2. Confusion matrices, ROC and PR curves

for model_name in ["LogisticRegression", "RandomForestClassifier", "LinearDiscriminantAnalysis"]:
    res = model_results[model_name]
    res["disp"].plot()
    plt.title(f"Confusion Matrix - {model_name}")
    plt.tight_layout()
    plt.show()

# ROC curves
plt.figure(figsize=(7, 5))
for model_name in ["LogisticRegression", "RandomForestClassifier", "LinearDiscriminantAnalysis"]:
    fpr, tpr = model_results[model_name]["roc_curve"]
    auc_val = comp_df.loc[comp_df.index.str.startswith(model_name), "AUC"].iloc[0]
    plt.plot(fpr, tpr, label=f"{model_name} (AUC={auc_val:.2f})")
plt.plot([0, 1], [0, 1], linestyle='--', color='grey')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.title('ROC Curves')
plt.legend()
plt.tight_layout()
plt.show()

# Precision-Recall curves
plt.figure(figsize=(7, 5))
for model_name in ["LogisticRegression", "RandomForestClassifier", "LinearDiscriminantAnalysis"]:
    precision, recall = model_results[model_name]["pr_curve"]
    ap = model_results[model_name]["average_precision"]
    plt.plot(recall, precision, label=f"{model_name} (AP={ap:.2f})")
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curves')
plt.legend()
plt.tight_layout()
plt.show()

"""**6. RELATIVE IMPORTANCE OF EACH PREDICTOR**

.feature_importances_ gives the importance of each feature, indicating how much it contributes to the model’s decision-making process.
"""

# Get feature importances from the random forest model (top 20)
random_forest_model = trained_models['RandomForestClassifier']
importances = random_forest_model.feature_importances_
feature_names = x_encoded.columns.tolist()
indices = np.argsort(importances)[::-1]
top_n = 20 if len(indices) >= 20 else len(indices)
top_idx = indices[:top_n]

plt.figure(figsize=(10, 6))
plt.title("Top Feature Importances (RandomForest)")
sorted_pairs = sorted([(feature_names[i], importances[i]) for i in top_idx], key=lambda t: t[1])
labels_sorted = [p[0] for p in sorted_pairs]
values_sorted = [p[1] for p in sorted_pairs]
plt.barh(labels_sorted, values_sorted, color="skyblue")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

"""**7. FiINE-TUNE KNN CLASSIFIER**

cross_val_knn: This function performs k-fold cross-validation on the KNN classifier. It shuffles the data indices to ensure random sampling for each fold. For each fold, it separates the data into training and validation sets, trains the KNN model, and evaluates its accuracy. Finally, it returns the mean accuracy across all folds.

K Values: k_values = range(1, 11) creates a list of values for
𝐾 ranging from 1 to 10. A loop iterates through each value of and calls the cross_val_knn function to compute the mean cross-validation accuracy for each
K. The mean accuracies are stored in the mean_scores list and printed.

Which K should be chosen? Even though K=9 gives the highest cross-validation accuracy (81.86%), it's important to consider K=5 as a strong candidate as well.

K = 5 yields an accuracy of 80.49%, which is already quite high and close to the maximum observed with 𝐾=9. From 𝐾=5 onward, the accuracy increases only marginally (and even decreases from K=5 to K=6). Since the difference between 𝐾=3 and K=9 is minimal, it might not justify the potential increase in complexity with larger K.

Smaller K values are more prone to overfitting because they capture more local variations, leading to higher variance. This is evident from the lower accuracy at K=1 (76.22%). Larger K values like K=10 may lead to smoother decision boundaries, but they also risk underfitting, especially if the underlying pattern in the data requires capturing more local details.

In this context, K=5 strikes a balance between capturing the structure of the data while avoiding overfitting. It smoothens out noisy local variations while keeping the model sufficiently flexible.
"""

# Function for cross-validation
y_train = np.asarray(y_train)

def cross_val_knn(x, y, k, num_folds=5, random_seed=72):
    np.random.seed(random_seed)
    fold_size = len(x) // num_folds
    indices = np.arange(len(x))
    np.random.shuffle(indices)  # Shuffle the indices
    accuracy_scores = []
    auc_scores = []

    for i in range(num_folds):
        # Create train and validation sets
        val_indices = indices[i * fold_size:(i + 1) * fold_size]
        train_indices = np.concatenate((indices[:i * fold_size], indices[(i + 1) * fold_size:]))

        # Use numpy indexing directly
        x_train_fold = x[train_indices]
        y_train_fold = y[train_indices]
        x_val_fold = x[val_indices]
        y_val_fold = y[val_indices]

        # Train the KNN classifier
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(x_train_fold, y_train_fold)

        # Predict and evaluate
        y_val_pred = knn.predict(x_val_fold)
        accuracy = accuracy_score(y_val_fold, y_val_pred)
        auc = roc_auc_score(y_val_fold, knn.predict_proba(x_val_fold)[:, 1])
        accuracy_scores.append(accuracy)
        auc_scores.append(auc)

    return np.mean(accuracy_scores), np.mean(auc_scores)

# Fine-tune KNN
k_values = range(1, 11)

for k in k_values:
    mean_accuracy, mean_auc = cross_val_knn(x_train_scaled, y_train, k)
    print(f'K={k}, Mean Cross-Validation Accuracy: {mean_accuracy:.4f}, Mean Cross-Validation AUC: {mean_auc:.4f}')

"""**8. FINE TUNE THE LOGISTIC MODEL**

We begin by defining a parameter grid, which includes the penalty type (L1 or L2 regularization) and different values of the regularization strength (C), ranging from 0.1 to 100.

A random seed is established to ensure reproducibility of the results. The code initializes a Logistic Regression model with the 'liblinear' solver, needed for the l1 regularization. The GridSearchCV is deployed and the verbosity level is set to 2 to provide detailed output during the fitting process.

The model is then fitted to the scaled training data using the grid search. After the fitting process is complete, the code prints the best hyperparameters found and the corresponding best cross-validation score, indicating how well the model performed during the grid search.

Next, the code utilizes the best model identified by the grid search to make predictions on the test set.
"""

# Define the parameter grid
param_grid = {
    'penalty': ['l1', 'l2'],
    'C': [0.1, 1, 10, 100]
}

# Set a random seed for reproducibility
np.random.seed(random_seed)

#logistic_model = trained_models['LogisticRegression']
logistic_model = LogisticRegression(solver='liblinear', random_state=random_seed)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=logistic_model, param_grid=param_grid,
                           scoring='accuracy', cv=5, verbose=2)

# Fit the model
grid_search.fit(x_train_scaled, y_train)

# Best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best Cross-Validation Score:", grid_search.best_score_)

# Predict on the test set using the best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(x_test_scaled)

# Evaluate the model
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""**9. PARAMETER VALUES AS C INCREASES**

9.1. results sorted in ascending order by rank_test_score, which allows to identidy the best-performing hyperparameter combination (l1, C=0.1).

9.2. plot to visualize the effect of the regularization strength parameter C on the performance of the Logistic Regression model, differentiated by penalty type (L1 and L2).
"""

#9.1

# Extract grid search results
results = pd.DataFrame(grid_search.cv_results_)
results = results[['param_penalty', 'param_C', 'mean_test_score', 'std_test_score', 'rank_test_score']]
print(results.sort_values(by='rank_test_score', ascending=True))

# 9.2

plt.figure(figsize=(10, 6))
# Loop through the penalties and plot the mean test scores for each value of C
for penalty in ['l1', 'l2']:
    subset = results[results['param_penalty'] == penalty]
    plt.plot(subset['param_C'], subset['mean_test_score'], marker='o', label=f'Penalty: {penalty}')

plt.title('Effect of C on Model Performance')
plt.xlabel('C Value')
plt.ylabel('Mean Cross-Validation Score')
plt.xscale('log')  # log scale for better visibility
# Use all unique C values for ticks, not just last subset
all_c_values = sorted(results['param_C'].unique(), key=lambda v: float(v))
plt.xticks(all_c_values)
plt.legend()
plt.grid()
plt.tight_layout()
plt.show()

"""**10. FIND BEST CLASSIFIER**

Interpretation of the results in the PDF Document
"""

# Define and fit the Logistic Regression model with L1 penalty and C=0.1 to find its AUC

# Set a random seed for reproducibility
np.random.seed(random_seed)

logistic_l1 = LogisticRegression(penalty='l1', C=0.1, solver='liblinear', random_state=random_seed)
logistic_l1.fit(x_train_scaled, y_train)
logistic_l1_prob = logistic_l1.predict_proba(x_test_scaled)[:, 1]
logistic_l1_auc = roc_auc_score(y_test, logistic_l1_prob)
print(f"Logistic Regression with L1 (C=0.1) AUC: {logistic_l1_auc}")